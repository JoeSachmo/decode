\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{marvosym}
\usepackage{enumerate}
%\usepackage{subfigure}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref} 
\usepackage{enumitem}


\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm




\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}
\DeclareMathOperator*{\argmax}{arg\,max}

\markboth{Homework 1}{Spring 2014 CS 475 Machine Learning: Homework 1} 

\begin{document}
\title{Assignment 2\\Decoding}
\author{King, Sean \and Lozano-Ramirez, Jacob}
\date{}


\maketitle
\section{Introduction}
Given a translation model and a language model, our task was to find the best way to decode a source sentence into the target language. In order to do this we constructed two search algorithms to sift through the vast search space, the first being a phrase based beam search with phrase swapping and the second being A* search. [INSERT MOTIVATION FOR BEAM SEARCH]. On the other hand, A* takes advantage of a heuristic function to determine which hypothesis is most likely to have maximum log probability on termination. Using both the current log probability of the hypothesis and its heuristic function we can order the hypotheses in a priority queue and update the most desirable first.
\section{Algorithm}
In order to search for the best hypothesis, a few things need to be initialized. The first of these is our heuristic function. The heuristic function used for this algorithm is a very simple maximized translation probability. For each position in the source sentence that has not been translated, we find the corresponding target word that gives the source word the highest probability. We then sum over the log probabilities of all such source words that are not in the coverage set to obtain the heuristic function for that coverage set.   \\\\
We then must initialize the potential search space for each coverage set. Since we are using phrased based translation, we must search the power set of potential french phrases. In order to do this while still keeping memory usage at a minimum, we index a list of tuples by the position in the source sentence such that the tuples represent all combinations of the source sentence that do not include the source word at the given index. In order to get the search space of a coverage set, one would only need to find the intersection with all indexes in the coverage set.\\\\
Now we need to find the optimal hypothesis for this translation. In order to do this, we iterate through all possible source phrases in the sentence that aren't in the coverage set of the current hypothesis. If they appear in the translation model, we then adjust the log probability of the current hypothesis using the translation and language models, updated the language model state, and updated the coverage set of the hypothesis. We then check this new hypothesis against those that are already in the priority queue. If there are no entries that have the same language model state and coverage set then we add the new hypothesis to the queue. As well, if there already exists an entry with the same language model state and coverage set, but our new hypothesis has a greater log probability, we replace the old hypothesis with our new one. However, if neither one of these states happen, we discard the new hypothesis. Finally, we update the current hypothesis by popping off the best hypothesis that is currently on the stack and update this hypothesis all over again. \\\\
This loop of hypothesis updates continues until we pop off a hypothesis whose coverage set is the same size as the source sentence length. This is therefore a closed hypothesis and the one that has the greatest probability.
\section{Results}
[UPDATE IN THE MORNING]
\end{document} 
